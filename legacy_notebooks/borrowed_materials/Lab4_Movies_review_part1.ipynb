{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is from chapter 7:\n",
    "    An Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido (O’Reilly). 2017 978-1-449-36941-5\n",
    "\n",
    "Find 'similar' code on github \n",
    "\n",
    "https://github.com/amueller/introduction_to_ml_with_python/blob/master/07-working-with-text-data.ipynb\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your data, unzip it. Discard the folder unsup for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(\"aclImdb_v1/aclImdb/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training data and display size of data and one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[6]:\n",
      "b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]:\\n{}\".format(text_train[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some bascis preprocessing on test data and on training data. Here you are simply removing the <tags>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "np.unique(y_train)\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and display size of data and one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"aclImdb_v1/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Create a binary representation of your text (change this to Tf-idf later )\n",
    "You might also want to do a lemmatization step before (see later steps)\n",
    "X_train is a sparse matrix (25000,7, y_ train is  the label of each text sample ( 25000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer().fit(text_train)\n",
    "X_train=vect.transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features: 74849\n"
     ]
    }
   ],
   "source": [
    "print(\"num features: {}\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 features: \n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"first 20 features: \\n{}\".format(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute '2f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ce711bd2a5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'mean cross val {.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute '2f'"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(max_iter=4000), X_train, y_train, cv = 5)\n",
    "print ('mean cross val {.2f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8824 0.876  0.8822 0.8884 0.8774]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print (y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2:(page 330-332) \n",
    "Fit (using training data )and score (using test data ) a logistic regression model and find best hyperpapameters for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3: (page 333-335)\n",
    "Check the options of calss CountVectorizer to remove \n",
    "a.words that dont appear often (min_df) b. Removing stopwords.\n",
    "Fit and score your model everytime to see if results have improved (also perform hyperparameter tuning)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: (page 336-338)\n",
    "Create Tf-If represenation of your data using the TfiffVectorizer. Explore the options of this class (i.e. min_df, norm,.....) \n",
    "Fit and score your logistic regression model and compare to the binary representation in step 2&3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 (page 339-344) Investigate the effect of the n-grams. \n",
    "Here experiment with bi-gram and tri-gram and compare to uni-gram model. \n",
    "Note you can run CountVectorizer and Tf_Idf with ngrams.\n",
    "It is always helpful to use pipelines to automate your exeperiments.\n",
    "You will need to practise this for your coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework:\n",
    "    1. Investigate how you can add to your pipeline a step of lemmatization or stemming and see if this type of normilization would improve results.\n",
    "    (hint check p. 344-347)\n",
    "    2. See how you automate your experiments by fitting your data and scoring it on different classifiers (possibly with different preprocessing, normalization, hyperparameters, evaluation metrics...etc) . \n",
    "    Hint you will need to use Phython pipelines ( see lecture notes and Applied Text Analysis with Phython chapters 4 &5)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
